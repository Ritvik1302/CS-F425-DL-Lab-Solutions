{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "4GCbtHWF01a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JXyQqtMGvasj"
      },
      "outputs": [],
      "source": [
        "text = \"Here is some example text data to train a word-level language model. It is a small dataset.\"\n",
        "\n",
        "words = text.lower().split()\n",
        "vocab = Counter(words)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_idx = {word: i for i, (word, _) in enumerate(vocab.items())}\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "\n",
        "encoded_text = [word_to_idx[word] for word in words]\n",
        "\n",
        "sequence_length = 5\n",
        "sequences = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(encoded_text) - sequence_length):\n",
        "    sequences.append(encoded_text[i:i+sequence_length])\n",
        "    targets.append(encoded_text[i+sequence_length])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(sequences, targets, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train, y_train = torch.tensor(X_train), torch.tensor(y_train)\n",
        "X_val, y_val = torch.tensor(X_val), torch.tensor(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WordLevelRNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_dim)\n",
        "        self.rnn = nn.RNN(hidden_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return hidden\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out, hidden\n"
      ],
      "metadata": {
        "id": "HdOOueQNvgNm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "input_size = vocab_size\n",
        "output_size = vocab_size\n",
        "hidden_dim = 128\n",
        "n_layers = 2\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "model = WordLevelRNN(input_size, output_size, hidden_dim, n_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        inputs = X_train[i:i+batch_size].to(device)\n",
        "        targets = y_train[i:i+batch_size].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        output, hidden = model(inputs)\n",
        "\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {total_loss/len(X_train):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME8O06nQvigx",
        "outputId": "55ec4a7c-e1ad-4e2f-a358-d581ea79e9fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100, Loss: 0.8964\n",
            "Epoch: 2/100, Loss: 0.5956\n",
            "Epoch: 3/100, Loss: 0.4033\n",
            "Epoch: 4/100, Loss: 0.2804\n",
            "Epoch: 5/100, Loss: 0.2008\n",
            "Epoch: 6/100, Loss: 0.1457\n",
            "Epoch: 7/100, Loss: 0.1067\n",
            "Epoch: 8/100, Loss: 0.0791\n",
            "Epoch: 9/100, Loss: 0.0599\n",
            "Epoch: 10/100, Loss: 0.0465\n",
            "Epoch: 11/100, Loss: 0.0370\n",
            "Epoch: 12/100, Loss: 0.0303\n",
            "Epoch: 13/100, Loss: 0.0253\n",
            "Epoch: 14/100, Loss: 0.0217\n",
            "Epoch: 15/100, Loss: 0.0188\n",
            "Epoch: 16/100, Loss: 0.0166\n",
            "Epoch: 17/100, Loss: 0.0149\n",
            "Epoch: 18/100, Loss: 0.0135\n",
            "Epoch: 19/100, Loss: 0.0123\n",
            "Epoch: 20/100, Loss: 0.0113\n",
            "Epoch: 21/100, Loss: 0.0105\n",
            "Epoch: 22/100, Loss: 0.0098\n",
            "Epoch: 23/100, Loss: 0.0091\n",
            "Epoch: 24/100, Loss: 0.0086\n",
            "Epoch: 25/100, Loss: 0.0081\n",
            "Epoch: 26/100, Loss: 0.0077\n",
            "Epoch: 27/100, Loss: 0.0073\n",
            "Epoch: 28/100, Loss: 0.0069\n",
            "Epoch: 29/100, Loss: 0.0066\n",
            "Epoch: 30/100, Loss: 0.0063\n",
            "Epoch: 31/100, Loss: 0.0060\n",
            "Epoch: 32/100, Loss: 0.0058\n",
            "Epoch: 33/100, Loss: 0.0056\n",
            "Epoch: 34/100, Loss: 0.0053\n",
            "Epoch: 35/100, Loss: 0.0051\n",
            "Epoch: 36/100, Loss: 0.0049\n",
            "Epoch: 37/100, Loss: 0.0048\n",
            "Epoch: 38/100, Loss: 0.0046\n",
            "Epoch: 39/100, Loss: 0.0044\n",
            "Epoch: 40/100, Loss: 0.0043\n",
            "Epoch: 41/100, Loss: 0.0041\n",
            "Epoch: 42/100, Loss: 0.0040\n",
            "Epoch: 43/100, Loss: 0.0039\n",
            "Epoch: 44/100, Loss: 0.0038\n",
            "Epoch: 45/100, Loss: 0.0036\n",
            "Epoch: 46/100, Loss: 0.0035\n",
            "Epoch: 47/100, Loss: 0.0034\n",
            "Epoch: 48/100, Loss: 0.0033\n",
            "Epoch: 49/100, Loss: 0.0032\n",
            "Epoch: 50/100, Loss: 0.0031\n",
            "Epoch: 51/100, Loss: 0.0031\n",
            "Epoch: 52/100, Loss: 0.0030\n",
            "Epoch: 53/100, Loss: 0.0029\n",
            "Epoch: 54/100, Loss: 0.0028\n",
            "Epoch: 55/100, Loss: 0.0027\n",
            "Epoch: 56/100, Loss: 0.0027\n",
            "Epoch: 57/100, Loss: 0.0026\n",
            "Epoch: 58/100, Loss: 0.0025\n",
            "Epoch: 59/100, Loss: 0.0025\n",
            "Epoch: 60/100, Loss: 0.0024\n",
            "Epoch: 61/100, Loss: 0.0024\n",
            "Epoch: 62/100, Loss: 0.0023\n",
            "Epoch: 63/100, Loss: 0.0023\n",
            "Epoch: 64/100, Loss: 0.0022\n",
            "Epoch: 65/100, Loss: 0.0022\n",
            "Epoch: 66/100, Loss: 0.0021\n",
            "Epoch: 67/100, Loss: 0.0021\n",
            "Epoch: 68/100, Loss: 0.0020\n",
            "Epoch: 69/100, Loss: 0.0020\n",
            "Epoch: 70/100, Loss: 0.0019\n",
            "Epoch: 71/100, Loss: 0.0019\n",
            "Epoch: 72/100, Loss: 0.0019\n",
            "Epoch: 73/100, Loss: 0.0018\n",
            "Epoch: 74/100, Loss: 0.0018\n",
            "Epoch: 75/100, Loss: 0.0017\n",
            "Epoch: 76/100, Loss: 0.0017\n",
            "Epoch: 77/100, Loss: 0.0017\n",
            "Epoch: 78/100, Loss: 0.0016\n",
            "Epoch: 79/100, Loss: 0.0016\n",
            "Epoch: 80/100, Loss: 0.0016\n",
            "Epoch: 81/100, Loss: 0.0016\n",
            "Epoch: 82/100, Loss: 0.0015\n",
            "Epoch: 83/100, Loss: 0.0015\n",
            "Epoch: 84/100, Loss: 0.0015\n",
            "Epoch: 85/100, Loss: 0.0014\n",
            "Epoch: 86/100, Loss: 0.0014\n",
            "Epoch: 87/100, Loss: 0.0014\n",
            "Epoch: 88/100, Loss: 0.0014\n",
            "Epoch: 89/100, Loss: 0.0014\n",
            "Epoch: 90/100, Loss: 0.0013\n",
            "Epoch: 91/100, Loss: 0.0013\n",
            "Epoch: 92/100, Loss: 0.0013\n",
            "Epoch: 93/100, Loss: 0.0013\n",
            "Epoch: 94/100, Loss: 0.0012\n",
            "Epoch: 95/100, Loss: 0.0012\n",
            "Epoch: 96/100, Loss: 0.0012\n",
            "Epoch: 97/100, Loss: 0.0012\n",
            "Epoch: 98/100, Loss: 0.0012\n",
            "Epoch: 99/100, Loss: 0.0012\n",
            "Epoch: 100/100, Loss: 0.0011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, word, hidden=None):\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.tensor([[word_to_idx[word]]]).to(device)\n",
        "\n",
        "    out, hidden = model(x)\n",
        "\n",
        "    prob = torch.softmax(out, dim=1).data\n",
        "    word_idx = torch.argmax(prob).item()\n",
        "\n",
        "    return idx_to_word[word_idx], hidden\n",
        "\n",
        "def generate_sequence(model, start_word, sequence_len=10):\n",
        "    model.eval()\n",
        "    words = [start_word]\n",
        "\n",
        "    hidden = model.init_hidden(1).to(device)\n",
        "\n",
        "    word = start_word\n",
        "    for _ in range(sequence_len - 1):\n",
        "        word, hidden = predict(model, word, hidden)\n",
        "        words.append(word)\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "start_word = 'here'\n",
        "print(\"Generated sequence:\", generate_sequence(model, start_word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG_arc2jvkZM",
        "outputId": "4c1a4a4e-ca3c-4e96-f0b5-bbfe3599554d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sequence: here word-level language model. it is word-level language model. it\n"
          ]
        }
      ]
    }
  ]
}